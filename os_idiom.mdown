# OS Idioms

## \#



## A
* Actor
* + non-blocking IO + IO multiplexing
* + Event demultiplexor
* + [ref](https://www.zhihu.com/question/26943938)
* + [ref](http://www.yeolar.com/note/2012/12/15/high-performance-io-design-patterns/)
* + Reactor
* + - epoll
* + Proactor
* + - IOCP

## B
* backpointer-based consistency (BBC)
* block corruption
* + checksum
* + - xor-based
* + - CRC (cyclic redundancy check)


## C
* conditional variable
* + wait
* + signal/signal_all
* + Mesa semantics (always while loops)
* crash consistency
* + data block (write / update)
* + inode (write / update)
* + bitmap (write / update)
* copy-on-write
* cache
* + performance
* + - hit time
* + - - small and simple cache
* + - - way prediction
* + - - trace cache
* + - cache bandwidth
* + - - nonblocking
* + - - multibanked
* + - - pipelined
* + - - multi-level cache
* + - - [ref](https://www.quora.com/What-is-meant-by-non-blocking-cache-and-multi-banked-cache)
* + - miss penalty
* + - - critical word first
* + - - merging write buffers
* + - miss rate
* + - - compiler optimization
* + - via parallelism
* + - - hardware prefetching
* + - - compiler prefetching
* cache consistency
* + update visibiility
* + stale cache
* + ping-pong cache
* + - same data
* + false-sharing
* + - data stored in same cacheline
* + transparent to programmer
* + MESI
* concurrency
* + MPI
* + OpenMP
* + oversubscription
* Critical section
* + Dekker
* + Peterson
* + Lamport Bakery
* + Szymanski
* + Eisenberg & McGuire
* CPU
* + [bugs](https://wiki.osdev.org/CPU_Bugs)


## D
* Dining Philosophers
* + global order acquire lock
* + global servicer
* + Chandy/Misra
* DMA
* + Memory-Device
* + Memory-Memory
* + [discussion](https://www.zhihu.com/question/266921250)
* Device driver
* disk scheduler
* + FCFS
* + SSTF
* + - only seek time (move between tracks)
* + Elevator(SCAN/C-SCAN)
* + - [ref](http://www.cnblogs.com/jianyungsun/archive/2011/03/16/1986439.html)
* + SPTF/SATF
* + - seek + rotation time
* distributed system
* + Resource sharing/Computational speedup/Reliability/Communication
* + reliable communication layers
* + - sender ->(meesage) receiver ->(ack) -> sender
* + - timeout -> dropped request / ack
* + DSM (distributed shared memory)
* + RPC (remote procedure call)
* + - Client/Server Model (C/S)
* + - stub generator (protocol compiler)
* + - - client stub
* + - - - create message buffer
* + - - - pack information
* + - - - send message to destination RPC server
* + - - - wait for reply
* + - - - unpack result
* + - - - return to caller
* + - - server stub
* + - - - unpack (unmarshaling/deserialization)
* + - - - call info actual function
* + - - - pack result
* + - - - send reply
* + - run-time library
* + - - naming
* + - - fragmentation
* + - - reassembly
* + - - byte ordering
* + - - - XDR (external data representation) / protobuf
* + - - synchronously
* + Naming Strategy
* + - Mount points (NFS)
* + - - location transparent, remote name can change
* + - - hard to maintain
* + - Global namespace (AFS/Sprite)
* + - - consistent
* + - - hard for caching, limit flexibility
* + [AFS-NFS-GFS](http://www.shuang0420.com/2016/12/10/Distributed%20Systems%E7%AC%94%E8%AE%B0%EF%BC%8DNFS%E3%80%81AFS%E3%80%81GFS/)
* + NFS
* + - [ref](https://csruiliu.github.io/blog/2017/11/17/nfs/)
* + - sharing
* + - centralized administration
* + - security
* + - simple and fast server crash recovery
* + - stateless protocol
* + - - shared state / distributed state -> complicating crash recovery
* + - idempotent opeartion
* + - client side caching
* + - flush on close
* + - block-aligned r/w
* + AFS
* + - [ref](https://csruiliu.github.io/blog/2017/11/17/afs/)
* + - [ref](http://blog.csdn.net/ak913/article/details/7197062)
* + - callback system
* + - polling
* + - last writer(closer) wins
* + GFS
* + - [ref](http://www.d-kai.me/google-file-system/)



## E
* Event-driven system
* + asynchronous IO
* + continuation based
* + signal/interrupt -> check complete
* + multi-core paging
* ECC (error correcting code)


## F
* futex
* + 2-phase lock
* + futex_wait/futex_wake
* file system
* + Application = Daemon = Servers = Shell
* + open file table
* + per-process file table
* + FAT (file allocation table)
* + NTFS
* + ext2-4
* + - super (metadata) + bitmap (data/inode) + inode + data
* + UFS (FFS)
* + - [ref](http://www.d-kai.me/ffs-unix%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%BC%BB%E7%A5%96/)
* + - increment block size
* + - fragment (subblock)
* + - block group (superblock, inode/data block) / Allocation group
* + - [ref](https://csruiliu.github.io/blog/2017/11/18/ffs/)
* + LFS (log-structured file system) / WAFL / ZFS / btrfs
* + - [ref](https://csruiliu.github.io/blog/2017/11/19/lfs/)
* + - reason
* + - - memory size ++
* + - - random IO << sequential IO
* + - - other poorly on common wordloads
* + - - not RAID-aware
* + - segment only with update
* + - write-buffering
* + - imap (inode map)
* + - shadow paging update
* + ZFS
* + - [ref](https://www.ibm.com/developerworks/cn/linux/l-zfs/index.html)
* + - [doc](https://docs.oracle.com/cd/E19253-01/819-7065/index.html)
* + - [ref](http://blog.csdn.net/wdy_yx/article/details/42848773)
* FSCK (file system checker)
* + check superblock
* + check inodes (free blocks) + inode state + inode link
* + duplicates
* + bad blocks
* + directory checks


## G
* GPU
* + [calculation](https://www.cnblogs.com/muchen/p/6138691.html)



## H
* Hard disk
* + SAS (Serial Attached SCSI)
* + SATA (Serial ATA)
* + [ref](https://www.webopedia.com/DidYouKnow/Computer_Science/sas_sata.asp)
* + [S.M.A.R.T.](https://en.wikipedia.org/wiki/SMART_criteria)
* + [Structure](https://zhuanlan.zhihu.com/p/32907254)
* hypervisors
* hazard pointer
* + [ref](http://blog.csdn.net/coryxie/article/details/8562627)


## I



## J
* journaling (write-ahead logging)
* + TID (transaction id)
* + data write
* + journal write (TxB metadata data)
* + journal commit (TxE)
* + checkpoint (write on-disk)
* + free journal

## K


## L
* LSE (latent-sector errors) / URE (unrecoverable read error)
* + RAID-DP
* lost write
* + write verify (read-after write)
* + ZFS checksum
* lock
* + mutex
* + shared_mutex (reader-write lock)
* + - upgrade_mutex
* + - [ref](https://oroboro.com/upgradable-read-write-locks/)
* + timed_mutex
* + recursive_mutex
* + deadlock
* + - bounded resources
* + - no preemption
* + - wait while holding
* + - circular waiting (not DAG)
* + - Solution: [Banker's Algorithm](https://zh.wikipedia.org/wiki/%E9%93%B6%E8%A1%8C%E5%AE%B6%E7%AE%97%E6%B3%95)
* + - - [banker-ref](http://www.cnblogs.com/Lynn-Zhang/p/5672080.html)
* + livelock: two thread restart each other
* lock-free
* + test-and-set 
* + compare-and-swap (CAS)
* + - DWCAS
* + fetch-and-add (tick lock)
* + load-linked/store-conditional MIPS/ARM
* + - will cause spurious failure
* + data structures
* + - [ref](https://liblfds.org/mediawiki/index.php?title=White_Papers)
* + ABA
* + - x = A -> B -> A => transparent to CAS
* + - add ABA counter
* LRU on Paging
* + perfect LRU
* + - Keep a time stamp for each page with the time of the last access. Throw out the LRU page. (expensive)
* + - Keep a list of pages, where the front of the list is the most recently used page, and the end is the least recently used. (double linked list)
* + Approximate LRU
* + - reference bits for each page
* + - additional reference bits
* + - - At regular intervals or on each memory access, shift the byte right, placing a 0 in the high order bit.
* + - - On a page fault, the lowest numbered page is kicked out
* + Second Change Algorithm
* + - frames in circular list
* + - page fault => check reference bit of next frame
* + - keep a modify bit
```
The reference bit and modify bit form a pair (r,m) where
1. (0,0) neither recently used nor modified - replace this page!
2. (0,1) not recently used but modified - not as good to replace, since the OS
must write out this page, but it might not be needed anymore.
3. (1,0) recently used and unmodified - probably will be used again soon, but
OS need not write it out before replacing it
4. (1,1) recently used and modified - probably will be used again soon and
the OS must write it out before replacing it
```
```
1. Page with (0,0) => replace the page.
2. Page with (0,1) => initiate an I/O to write out the page, locks the page in
memory until the I/O completes, clears the modified bit, and continue the
search
3. For pages with the reference bit set, the reference bit is cleared.
4. If the hand goes completely around once, there was no (0,0) page.
    • On the second pass, a page that was originally (0,1) or (1,0) might
    have been changed to (0,0) => replace this page
    • If the page is being written out, waits for the I/O to complete and then remove the page.
    • A (0,1) page is treated as on the first pass.
    • By the third pass, all the pages will be at (0,0)
```
* + Multihreading replacement
* + - thrashing: over-commiteed memory and pages are continuously tossed
* + - proportional allocation: more page frames to large process
* + - global replacement: put all pages from all processes in one pool
* + - per-process: each process has its own pool of pages
* + Clock-PRO


## M
* mutex
* misdirected write
* + PID (physical id)
* monitor
* + [ref](https://www.jianshu.com/p/8b3ed769bc9f)
* + Mesa
* + Hoare


## N
* Network
* + Application layer: applications that use the net, e.g., mail, netscape, Xservices, ftp, telnet, provide a UI
* + Presentation layer: data format conversion, e.g., big/little endian integer format)
* + Session layer: implements the communication strategy, such as RPC. Provided by libraries.
* + Transport layer: reliable end-to-end communication between any set of nodes. Provided by OS.
* + Network layer: routing and congestion control. Usually implemented in OS.
* + Data Link Control layer: reliable point-to-point communication of packets over an unreliable channel. Sometimes implemented in hardware, sometimes in software (PPP).
* + Physical layer: electrical/optical signaling across a “wire”. Deals with timing issues. Implemented in hardware.


## O
* order / cache consistency / memory coherence
* + reorder
* + - compiler reorder
* + - - `__asm__ __volatile__ (""::: "memory")` ensure compiler on sequence-before order
* + - CPU reorder
* + - - X86: `__asm__ __volatile__ ("mfence"::: "memory")` (lfence/sfence) ensure CPU on happens-before order
* + multicore cache protocal (MESI)
* + [ref](https://www.zhihu.com/question/24301047)
* + memory order <m (不同内存的读/写 != CPU执行 != 程序顺序(program order <p)不同)
* + - R(L) -> R
* + - R -> W(S)
* + - W -> R
* + - - store buffer waiting for response, so reorder faster
* + - W -> W
* + - 同一内存 atomic_operation => sequenced-before
* + - 单核(Logic Core) => transparent
* + - 不同内存 atomic_operation => 乱序
* + Relaxed
* + - TSO (total store order): relax W -> R [x86]
* + - PSO (partial store order): relax W -> W
* + - Weak ordering / Power PC / Release: relax R -> W / R -> R
* + sequenced-before => single thread happens-before
* + happens-before (strict partial order)
* + - inter-thread happens-before
* + - - A synchronizes-with B
* + - - A is dependency-ordered before B
* + - - A synchronizes-with some evaluation X, and X is sequenced-before B
* + - - A is sequenced-before some evaluation X, and X inter-thread happens-before B
* + - - A inter-thread happens-before some evaluation X, and X inter-thread happens-before B
* + - sequenced-befire
* + - dependency-ordered-before
* + synchronized-with
* + visible side-effects
* + - happens-before
* + - no middle other side effect
* + seq_cst: SC
* + - every atomic operation is not reordered between thread
* + - Atomic operations tagged memory_order_seq_cst not only order memory the same way as release/acquire ordering (everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load), but also establish a single total modification order of all atomic operations that are so tagged.
* + relaxed: Relaxed
* + - atomic on specific variable not reordered in single thread
* + - Atomic operations tagged memory_order_relaxed are not synchronization operations; they do not impose an order among concurrent memory accesses. They only guarantee atomicity and modification order consistency.
* + acq_rel/consume/acquire/release: Acquire-Release
* + - release synchronized-with acquire
* + - side effect: write before release visible to read after acquire
* + - If an atomic store in thread A is tagged memory_order_release and an atomic load in thread B from the same variable is tagged memory_order_acquire, all memory writes (non-atomic and relaxed atomic) that happened-before the atomic store from the point of view of thread A, become visible side-effects in thread B, that is, once the atomic load is completed, thread B is guaranteed to see everything thread A wrote to memory.
* + consume: dependency-ordered-before
* + - release synchronized-with consume
* + - If an atomic store in thread A is tagged memory_order_release and an atomic load in thread B from the same variable is tagged memory_order_consume, all memory writes (non-atomic and relaxed atomic) that are dependency-ordered-before the atomic store from the point of view of thread A, become visible side-effects within those operations in thread B into which the load operation carries dependency, that is, once the atomic load is completed, those operators and functions in thread B that use the value obtained from the load are guaranteed to see what thread A wrote to memory.
* + memory_order_relaxed	
* + - Relaxed operation: there are no synchronization or ordering constraints imposed on other reads or writes, only this operation's atomicity is guaranteed (see Relaxed ordering below)
* + memory_order_consume	
* + - A load operation with this memory order performs a consume operation on the affected memory location: no reads or writes in the current thread dependent on the value currently loaded can be reordered before this load. Writes to data-dependent variables in other threads that release the same atomic variable are visible in the current thread. On most platforms, this affects compiler optimizations only (see Release-Consume ordering below)
* + memory_order_acquire	
* + - A load operation with this memory order performs the acquire operation on the affected memory location: no reads or writes in the current thread can be reordered before this load. All writes in other threads that release the same atomic variable are visible in the current thread (see Release-Acquire ordering below)
* + - Ensure LL/LS (RR/RW)
* + memory_order_release	
* + - A store operation with this memory order performs the release operation: no reads or writes in the current thread can be reordered after this store. All writes in the current thread are visible in other threads that acquire the same atomic variable (see Release-Acquire ordering below) and writes that carry a dependency into the atomic variable become visible in other threads that consume the same atomic (see Release-Consume ordering below).
* + - Ensure LS/SS (RW/WW)
* + memory_order_acq_rel	
* + - A read-modify-write operation with this memory order is both an acquire operation and a release operation. No memory reads or writes in the current thread can be reordered before or after this store. All writes in other threads that release the same atomic variable are visible before the modification and the modification is visible in other threads that acquire the same atomic variable.
* + memory_order_seq_cst	
* + - A load operation with this memory order performs an acquire operation, a store performs a release operation, and read-modify-write performs both an acquire operation and a release operation, plus a single total order exists in which all threads observe all modifications in the same order (see Sequentially-consistent ordering below)
* + atomic variable-variable or fence-fence
* + lock ensures read-acquire + write-release (lock-acquire / unlock-release) (AKA memory fence)



## P
* preemptive
* PIO (programmed IO)
* parallelism
* PCIE
* + [ref](https://zhuanlan.zhihu.com/p/34047690)


## Q



## R
* RAID
* + [ref](https://zh.wikipedia.org/wiki/RAID)
* + RAID0: Striping
* + RAID1: Mirroring
* + RAID4: Saving space with parity
* + RAID5: rotating parity
* RCU (read-copy-update)
* RAM
* + SRAM
* + - cache
* + DRAM
* + EEPROM
* + Flash
* + ECC
* + Odd-Even check bit
* + Chipkill
* 


## S
* scheduler
* + affinity
* + FCFS
* + - First-Come-First-Served (or FIFO: First-In-First-Out)
* + - average wait time is highly variable as short jobs may wait behind long jobs
* + - may lead to poor overlap of I/O and CPU since CPU-bound processes will force I/O bound processes to wait for the CPU, leaving the I/O devices idle
* + Round-robin
* + - Use a time slice and preemption to alternate jobs
* + - Average waiting time can be bad
* + SJF STCF SRTF(shortest remaining time first, only for preemptive)
* + - Shortest Job First
* + - provably optimal w.r.t. minimize average waiting time
* + - works for preemptive & non-preemptive schedulers
* + - IO bound jobs get priority over CPU bound jobs
* + - impossible to predict time
* + - long running CPU bound job can starve
* + - optimal but unfair
* + MLFQ (Windows NT)
* + - RR on each priority queue and change priority
* + - job's time slices expires => drop 1 level
* + - context swiching from I/O bound => up to top priorit
* + Lottery Scheduling
* + - Jobs get tickets and scheduler randomly picks winning ticket
* + - assign most to short running jobs, fewer to long running job, avoid starvation
* + - fair scheduling
* + multi-core
* + - SQMS
* + - MQMS
* + - O(1)
* + - CFS
* + - BFS
* spinlock / lock-free
* + test-and-set 
* + compare-and-swap (CAS)
* + - DWCAS
* + fetch-and-add (tick lock)
* + load-linked/store-conditional MIPS/ARM
* + - will cause spurious failure
* semaphore
* + sem_wait (-1 wait if negative otherwise run)
* + sem_post (+1 wake if 1 or more waiting)
* + 
* soft-update

## T
* TLB
* + hardware-handling
* + software-handling 
* + - [ref](https://en.wikipedia.org/wiki/Translation_lookaside_buffer#TLB-miss_handling)
* + - [another-ref-says-RISC-and-IA-64](http://www.informit.com/articles/article.aspx?p=29961&seqNum=4)
* + PCID/ASID/PID
* Thread
* + kernel thread (1:1)
* + - lightweight process
* + - context switching
* + user thread (m:1)
* + - no context switching by kernel (but by user)
* + - scheduling hard, affinity problem
* + - must have idle thread process
* + m:1 model
* + - 1:1 NPTL/LinuxThreads (kernel thread)
* + - - [NPTL-vs-LinuxThreads](https://www.ibm.com/developerworks/cn/linux/l-threading.html)
* + Scheduling Algorithm
* + - CPU Utilization: percentage of time of CPU busy
* + - Throughput: number of processes completed / unit time
* + - Turnaround time: length of time it rakes to run a process from init to termination, including all waiting time, turnaroundTime = burstTime + waitingTime = finishTime - arrivalTime
* + - Waiting time: total amount of time the a process in the ready queue, waitingTime = startTime - arrivalTime
* + - - give each process the same amount of time on the processor. This might actually increase average response time.
* + - Response time: time between when a process is ready to run and its next I/O request
* + - - provide output to the user as quickly as possible and process their input as soon as it is received.
* + - - in interactive systems, predictability may be more important than a low average with a high variance.


## U

## V
* Virtualization
* + CPU
* + - scheduler
* + - limited direct execution
* + Memory
* + - address translation
* + - segmentation
* + - virtual <=> physical
* + - - paging
* + - - TLB
* + VMM
* + I/O
* VMM (virutal machine monitor)
* + [ref](http://blog.csdn.net/flyforfreedom2008/article/details/45113635)
* + CPU: limited direct execution
* + - machine switch
* + - privileged operation
* + - trap => OS trap trampoline
* + Memory
* + - VM page table => VPN-to-MFN
* + - software handled TLB: VM TLB handler => OS TLB handler
* + - hardware handled TLB: shadow page table
* + information gap
* + - idle loop
* + - zeroing of page table
* + I/O
* + para-virtualization


## W



## X



## Y



## Z

